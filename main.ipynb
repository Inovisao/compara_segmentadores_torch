{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully.\n",
      "dict_keys(['epoch', 'model_state_dict'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "path_checkpoints = 'Evaluation_from_checkpoints/checkpoints/model_checkpoints/1_deeplabv3_resnet101_adagrad.pth'\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load(path_checkpoints, map_location=torch.device('cuda:0'))\n",
    "    print(\"Checkpoint loaded successfully.\")\n",
    "    print(checkpoint.keys())\n",
    "except RuntimeError as e:\n",
    "    print(\"Failed to load checkpoint:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pasta com os checkpoints (*.pth):  /home/corbusier/development/compara_segmentadores_torch_/checkpoints\n"
     ]
    }
   ],
   "source": [
    "import src.architectures as architectures\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = architectures.segmentation.deeplabv3_resnet101(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.55s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class COCOMultiLabelSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, include_patterns=None, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.coco = COCO(annotation_file)\n",
    "        \n",
    "        # Filter image IDs based on the include_patterns\n",
    "        self.image_ids = []\n",
    "        for image_id in self.coco.imgs.keys():\n",
    "            img_info = self.coco.loadImgs(image_id)[0]\n",
    "            img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "            if os.path.exists(img_path):\n",
    "                if include_patterns:\n",
    "                    if any(pattern in img_info['file_name'] for pattern in include_patterns):\n",
    "                        self.image_ids.append(image_id)\n",
    "                else:\n",
    "                    self.image_ids.append(image_id)\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        img_info = self.coco.loadImgs(image_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Create an empty mask\n",
    "        mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "\n",
    "        for ann in anns:\n",
    "            mask = np.maximum(mask, self.coco.annToMask(ann) * (ann['category_id'] + 1))\n",
    "\n",
    "        mask = Image.fromarray(mask)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "# Define any transformations (e.g., resizing, normalization) to apply to the images and masks\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create an instance of the dataset\n",
    "root_dir = '/home/corbusier/development/compara_segmentadores_torch_/data_512_UCDB_UPS/all/imagens'\n",
    "annotation_file = '/home/corbusier/development/compara_segmentadores_torch_/data_512_UCDB_UPS/annotations_coco_json/_annotations.coco.json'\n",
    "# include_patterns = ['UPS', 'UCDB']\n",
    "include_patterns = ['UPS']\n",
    "\n",
    "dataset = COCOMultiLabelSegmentationDataset(root_dir=root_dir, annotation_file=annotation_file, include_patterns=include_patterns, transform=image_transform)\n",
    "\n",
    "# Create a DataLoader for your dataset\n",
    "val_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Example usage: iterate over the DataLoader\n",
    "for images, masks in val_loader:\n",
    "    # Perform your evaluation here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device, num_classes):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(1)  # Remove the extra dimension in labels\n",
    "            outputs = model(images)['out']\n",
    "            preds = torch.argmax(outputs, dim=1)  # Convert to discrete class labels\n",
    "            \n",
    "            # Print shapes for debugging\n",
    "            print(f'Batch {i+1}: images shape: {images.shape}, labels shape: {labels.shape}, preds shape: {preds.shape}')\n",
    "            \n",
    "            # Move to CPU and convert to numpy arrays\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Print shapes for debugging\n",
    "    print(f'All preds shape: {all_preds.shape}, all labels shape: {all_labels.shape}')\n",
    "    print(f'Unique labels: {np.unique(all_labels)}, unique preds: {np.unique(all_preds)}')\n",
    "    \n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 2: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 3: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 4: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 5: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 6: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 7: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 8: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 9: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 10: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 11: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 12: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 13: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 14: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 15: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 16: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 17: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 18: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 19: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 20: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 21: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 22: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 23: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 24: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 25: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 26: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 27: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 28: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 29: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 30: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 31: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 32: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 33: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 34: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 35: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 36: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 37: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 38: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 39: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 40: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 41: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 42: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 43: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 44: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 45: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 46: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 47: images shape: torch.Size([16, 3, 512, 512]), labels shape: torch.Size([16, 512, 512]), preds shape: torch.Size([16, 512, 512])\n",
      "Batch 48: images shape: torch.Size([2, 3, 512, 512]), labels shape: torch.Size([2, 512, 512]), preds shape: torch.Size([2, 512, 512])\n",
      "All preds shape: (754, 512, 512), all labels shape: (754, 512, 512)\n",
      "Unique labels: [0.         0.00392157 0.00784314 0.01568628], unique preds: [ 0  2  3  4  5  7  9 13 14 16 17]\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Number of classes (background, vegetation, grass)\n",
    "num_classes = 3\n",
    "\n",
    "# Get predictions and true labels\n",
    "preds, labels = evaluate(model, val_loader, device, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened preds shape: (197656576,), flattened labels shape: (197656576,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.3333333333333333\n",
      "Recall: 9.520553467444463e-05\n",
      "F1-score: 0.0001903567003145923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/corbusier/miniconda3/envs/seg_comp/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Precision: [1. 0. 0.]\n",
      "Class Recall: [0.00028562 0.         0.        ]\n",
      "Class F1-score: [0.00057107 0.         0.        ]\n",
      "mIoU: 6.994274232775805e-05\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Flatten arrays\n",
    "preds_flat = preds.flatten()\n",
    "labels_flat = labels.flatten()\n",
    "\n",
    "# Print flattened shapes for debugging\n",
    "print(f'Flattened preds shape: {preds_flat.shape}, flattened labels shape: {labels_flat.shape}')\n",
    "\n",
    "# Filter out invalid labels (e.g., labels not in [0, num_classes-1])\n",
    "valid_mask = (labels_flat >= 0) & (labels_flat < num_classes)\n",
    "preds_flat = preds_flat[valid_mask]\n",
    "labels_flat = labels_flat[valid_mask]\n",
    "\n",
    "# Ensure labels are integers\n",
    "preds_flat = preds_flat.astype(int)\n",
    "labels_flat = labels_flat.astype(int)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(labels_flat, preds_flat, average='macro', labels=np.arange(num_classes))\n",
    "recall = recall_score(labels_flat, preds_flat, average='macro', labels=np.arange(num_classes))\n",
    "f1 = f1_score(labels_flat, preds_flat, average='macro', labels=np.arange(num_classes))\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-score: {f1}')\n",
    "\n",
    "# Calculate class-specific metrics\n",
    "class_precision = precision_score(labels_flat, preds_flat, average=None, labels=np.arange(num_classes))\n",
    "class_recall = recall_score(labels_flat, preds_flat, average=None, labels=np.arange(num_classes))\n",
    "class_f1 = f1_score(labels_flat, preds_flat, average=None, labels=np.arange(num_classes))\n",
    "\n",
    "print(f'Class Precision: {class_precision}')\n",
    "print(f'Class Recall: {class_recall}')\n",
    "print(f'Class F1-score: {class_f1}')\n",
    "\n",
    "# Calculate mIoU\n",
    "def mean_iou(pred, true, num_classes):\n",
    "    ious = []\n",
    "    for cls in range(num_classes):\n",
    "        pred_cls = pred == cls\n",
    "        true_cls = true == cls\n",
    "        intersection = np.logical_and(pred_cls, true_cls).sum()\n",
    "        union = np.logical_or(pred_cls, true_cls).sum()\n",
    "        if union == 0:\n",
    "            ious.append(float('nan'))  # If there is no ground truth, do not include in mean IoU\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "            ious.append(iou)\n",
    "    return np.nanmean(ious)\n",
    "\n",
    "miou = mean_iou(preds, labels, num_classes)\n",
    "print(f'mIoU: {miou}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg_comp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
